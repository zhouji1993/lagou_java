{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## projec info \n",
    " * @Description:\n",
    " * 需求:将http.log文件中的ip转换为地址。如将 122.228.96.111 转为温州，并统计各城市的总访问量\n",
    " * http.log数据格式：时间戳、IP地址、访问网址、访问数据、浏览器信息\n",
    " * http.log数据样例：20090121000132095572000|125.213.100.123|show.51.com|/shoplist.php?phpfile=shoplist2.php&style=1&sex=137|Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; Mozilla/4.0(Compatible Mozilla/4.0(Compatible-EmbeddedWB 14.59 http://bsalsa.com/ EmbeddedWB- 14.59  from: http://bsalsa.com/ )|http://show.51.com/main.php|\n",
    " * @Author zj\n",
    " * @Time\n",
    " * @code python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import HiveContext,SparkSession\n",
    "from pyspark.sql.functions import split,row_number\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"Feature Engineering\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.default.parrallelism\", \"4\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel('warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_id_rdd = sc.textFile(r\"data/ip.dat\").map(lambda x: x.split(\"|\")).map(\n",
    "        lambda x: (x[2], x[3],x[7], x[13], x[14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_broadcast = sc.broadcast(city_id_rdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_data = sc.textFile(r\"data/http.log\").map(\n",
    "        lambda x: x.split(\"|\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将IP地址转化为32位2进制数字\n",
    "def ip_transform(ip):\n",
    "    ips = ip.split(\".\")  # [223,243,0,0] 32位二进制数\n",
    "    ip_num = 0\n",
    "    for i in ips:\n",
    "        ip_num = int(i) | ip_num << 8\n",
    "    return ip_num\n",
    "\n",
    "\n",
    "# 二分法查找ip对应的行的索引\n",
    "def binary_search(ip_num, broadcast_value):\n",
    "    start = 0\n",
    "    end = len(broadcast_value) - 1\n",
    "    while (start <= end):\n",
    "        mid = int((start + end) / 2)\n",
    "        if ip_num >= int(broadcast_value[mid][0]) and ip_num <= int(broadcast_value[mid][1]):\n",
    "            return mid\n",
    "        if ip_num < int(broadcast_value[mid][0]):\n",
    "            end = mid\n",
    "        if ip_num > int(broadcast_value[mid][1]):\n",
    "            start = mid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pos(x):\n",
    "        city_broadcast_value = city_broadcast.value\n",
    "\n",
    "        # 根据单个ip获取对应经纬度信息\n",
    "        def get_result(ip):\n",
    "            ip_num = ip_transform(ip)\n",
    "            index = binary_search(ip_num, city_broadcast_value)\n",
    "            # ((地区),1)\n",
    "            return ((city_broadcast_value[index][2]), 1)\n",
    "\n",
    "        x = map(tuple, [get_result(ip) for ip in x])\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('重庆', 868), ('石家庄', 383), ('昆明', 126), ('北京', 1535), ('西安', 1824)]\n"
     ]
    }
   ],
   "source": [
    "# 类似map,但是map是一条一条传给里面函数\n",
    "# mapPartitions数据是一部分一部分传给函数的\n",
    "dest_rdd = dest_data.mapPartitions(lambda x: get_pos(x))  # ((地点),1)\n",
    "\n",
    "result_rdd = dest_rdd.reduceByKey(lambda a, b: a + b)\n",
    "print(result_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(ip_num, broadcast_value):\n",
    "    start = 0\n",
    "    end = len(broadcast_value) - 1\n",
    "    while (start <= end):\n",
    "        mid = int((start + end) / 2)\n",
    "        if ip_num >= int(broadcast_value[mid][0]) and ip_num <= int(broadcast_value[mid][1]):\n",
    "            return mid\n",
    "        if ip_num < int(broadcast_value[mid][0]):\n",
    "            end = mid\n",
    "        if ip_num > int(broadcast_value[mid][1]):\n",
    "            start = mid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_transform(ip):\n",
    "    ips = ip.split(\".\")  # [223,243,0,0] 32位二进制数\n",
    "    ip_num = 0\n",
    "    for i in ips:\n",
    "        ip_num = int(i) | ip_num << 8\n",
    "    return ip_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# project 2 日志分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- project info \n",
    "\n",
    "日志格式：IP\t命中率(Hit/Miss)\t响应时间\t请求时间\t请求方法\t请求URL\t请求协议\t状态码\t响应大小referer 用户代理\n",
    "\n",
    "日志文件位置：data/cdn.txt\n",
    "\n",
    "- 数据格式:\n",
    "\n",
    "\n",
    "`100.79.121.48 HIT 33 [15/Feb/2017:00:00:46 +0800] \"GET http://cdn.v.abc.com.cn/videojs/video.js HTTP/1.1\" 200 174055 \"http://www.abc.com.cn/\" \"Mozilla/4.0+(compatible;+MSIE+6.0;+Windows+NT+5.1;+Trident/4.0;)\"`\n",
    "   - 术语解释：\n",
    "\n",
    "      *  PV(page view)，即页面浏览量；衡量网站或单一网页的指标\n",
    "\n",
    "      * uv(unique visitor)，指访问某个站点或点击某条新闻的不同IP地址的人数\n",
    "\n",
    "   - 任务:\n",
    "\n",
    "    - 2.1、计算独立IP数\n",
    "\n",
    "    - 2.2、统计每个视频独立IP数（视频的标志：在日志文件的某些可以找到 *.mp4，代表一个视频文件）\n",
    "\n",
    "    - 2.3、统计一天中每个小时的流量\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关需要使用的包\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark \n",
    "\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import HiveContext,SparkSession\n",
    "from pyspark.sql.functions import split,row_number\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "创建spark session APPname 是project2\n",
    "\"\"\"\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Project2\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.default.parrallelism\", \"4\") \\\n",
    "    .config(\"spark.sql.execution.arrow.enabled\", \"ture\")\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel('warn')\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"warn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sc.textFile(r\"data/cdn.txt\").map(lambda x: x.split(\" \")).map(lambda x:(x[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('114.55.227.102', 9348),\n",
       " ('220.191.255.197', 2640),\n",
       " ('115.236.173.94', 2476),\n",
       " ('183.129.221.102', 2187),\n",
       " ('112.53.73.66', 1794),\n",
       " ('115.236.173.95', 1650),\n",
       " ('220.191.254.129', 1278),\n",
       " ('218.88.25.200', 751),\n",
       " ('183.129.221.104', 569),\n",
       " ('115.236.173.93', 529)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 独立IP访问量前10位\n",
    "inputs.reduceByKey(lambda a, b: a + b).sortBy(keyfunc=(lambda x:x[1]),ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新读取文件\n",
    "input2 = sc.textFile(r\"data/cdn.txt\").map(lambda x: x.split(\" \")).map(lambda x:(x[6],x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "restr= re.compile(\"\"\".+/(\\S+\\.mp4)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http://cdn.v.abc.com.cn/141011.mp4', '115.192.186.231'),\n",
       " ('http://cdn.v.abc.com.cn/140987.mp4', '125.105.41.123'),\n",
       " ('http://cdn.v.abc.com.cn/140987.mp4', '211.138.116.41'),\n",
       " ('http://cdn.v.abc.com.cn/140853.mp4', '60.181.47.255'),\n",
       " ('http://cdn.v.abc.com.cn/140987.mp4', '115.192.186.231')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2.filter(lambda x: restr.match(x[0])).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('141081.mp4', 2393),\n",
       " ('140995.mp4', 2050),\n",
       " ('141027.mp4', 1784),\n",
       " ('141090.mp4', 1702),\n",
       " ('141032.mp4', 1528),\n",
       " ('89973.mp4', 1522),\n",
       " ('141080.mp4', 1425),\n",
       " ('141035.mp4', 1321),\n",
       " ('141082.mp4', 1272),\n",
       " ('140938.mp4', 814)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每个视频独立IP数\n",
    "# 先将实际的视频截取出来，然后再进行groupByKey统计\n",
    "input2.filter(lambda x: restr.match(x[0]))\\\n",
    ".map(lambda x: (x[0].split(\"//\")[1].split(\"/\")[1],x[1]))\\\n",
    ".groupByKey()\\\n",
    ".mapValues(lambda x:len(set(x)))\\\n",
    ".sortBy(keyfunc=(lambda x:x[1]),ascending=False)\\\n",
    ".take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新读取文件\n",
    "input3 = sc.textFile(r\"data/cdn.txt\").map(lambda x: x.split(\" \")).map(lambda x:(x[3].split(\":\")[1],int(x[9])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('00', '15.453603968955576G'),\n",
       " ('01', '3.905158909969032G'),\n",
       " ('02', '5.195773116312921G'),\n",
       " ('03', '3.6369203999638557G'),\n",
       " ('04', '3.2884229784831405G'),\n",
       " ('05', '5.021793280728161G'),\n",
       " ('06', '11.671565165743232G'),\n",
       " ('07', '22.00996015779674G'),\n",
       " ('08', '43.230181308463216G'),\n",
       " ('09', '52.83912879694253G'),\n",
       " ('10', '61.57396282441914G'),\n",
       " ('11', '45.854314849711955G'),\n",
       " ('12', '47.00132285710424G'),\n",
       " ('13', '51.02019470091909G'),\n",
       " ('14', '55.84231084771454G'),\n",
       " ('15', '45.46238579135388G'),\n",
       " ('16', '45.94618751946837G'),\n",
       " ('17', '44.81707710213959G'),\n",
       " ('18', '45.33248774614185G'),\n",
       " ('19', '51.932280966080725G'),\n",
       " ('20', '55.40267270896584G'),\n",
       " ('21', '53.74120971187949G'),\n",
       " ('22', '42.061507997103035G'),\n",
       " ('23', '25.0245084669441G')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input3.reduceByKey(lambda a, b: a + b).mapValues(lambda x: str(x/1024/1024/1024) + \"G\").sortBy(keyfunc=(lambda x:x[0])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3、Spark面试题**\n",
    "\n",
    "假设点击日志文件(click.log)中每行记录格式如下：\n",
    "\n",
    "~~~\n",
    "INFO 2019-09-01 00:29:53 requestURI:/click?app=1&p=1&adid=18005472&industry=469&adid=31\n",
    "INFO 2019-09-01 00:30:31 requestURI:/click?app=2&p=1&adid=18005472&industry=469&adid=31\n",
    "INFO 2019-09-01 00:31:03 requestURI:/click?app=1&p=1&adid=18005472&industry=469&adid=32\n",
    "INFO 2019-09-01 00:31:51 requestURI:/click?app=1&p=1&adid=18005472&industry=469&adid=33\n",
    "~~~\n",
    "\n",
    "另有曝光日志(imp.log)格式如下：\n",
    "\n",
    "~~~\n",
    "INFO 2019-09-01 00:29:53 requestURI:/imp?app=1&p=1&adid=18005472&industry=469&adid=31\n",
    "INFO 2019-09-01 00:29:53 requestURI:/imp?app=1&p=1&adid=18005472&industry=469&adid=31\n",
    "INFO 2019-09-01 00:29:53 requestURI:/imp?app=1&p=1&adid=18005472&industry=469&adid=34\n",
    "~~~\n",
    "\n",
    "3.1、用Spark-Core实现统计每个adid的曝光数与点击数，将结果输出到hdfs文件；\n",
    "\n",
    "输出文件结构为adid、曝光数、点击数。注意：数据不能有丢失（存在某些adid有imp，没有clk；或有clk没有imp）\n",
    "\n",
    "3.2、你的代码有多少个shuffle，是否能减少？\n",
    "\n",
    "（提示：仅有1次shuffle是最优的）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse as urlpar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀入數據\n",
    "clickLog = sc.textFile(\"data/click.log\")\n",
    "impLog = sc.textFile(\"data/imp.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "clkRDD = clickLog.map(lambda line: line.split(\" \")).map(lambda x: urlpar.urlparse(x[3])).map(lambda x: (urlpar.parse_qs(x.query)['adid'][1],(1,0) ))\n",
    "impRDD= impLog.map(lambda line: line.split(\" \")).map(lambda x: urlpar.urlparse(x[3])).map(lambda x: (urlpar.parse_qs(x.query)['adid'][1],(0,1) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('31', (2, 2)), ('32', (1, 0)), ('33', (1, 0)), ('34', (0, 1))]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clkRDD.union(impRDD).reduceByKey(lambda x , y : (x[0] + y[0] , x[1] + y[1] ) ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('31', (0, 1)), ('31', (0, 1))]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impLog.map(lambda line: line.split(\" \")).map(lambda x: urlpar.urlparse(x[3])).map(lambda x: (urlpar.parse_qs(x.query)['adid'][1],(0,1) )).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('31', (1, 0)), ('31', (1, 0))]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clickLog.map(lambda line: line.split(\" \")).map(lambda x: urlpar.urlparse(x[3])).map(lambda x: (urlpar.parse_qs(x.query)['adid'][1],(1,0) )).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# project 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6、A表有三个字段：ID、startdate、enddate，有3条数据：\n",
    "\n",
    "~~~\n",
    "1 2019-03-04 2020-02-03\n",
    "2 2020-04-05 2020-08-04\n",
    "3 2019-10-09 2020-06-11\n",
    "~~~\n",
    "\n",
    "写SQL（需要SQL和DSL）将以上数据变化为：\n",
    "\n",
    "~~~\n",
    "2019-03-04\t2019-10-09\n",
    "2019-10-09\t2020-02-03\n",
    "2020-02-03\t2020-04-05\n",
    "2020-04-05\t2020-06-11\n",
    "2020-06-11\t2020-08-04\n",
    "2020-08-04\t2020-08-04\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"1 2019-03-04 2020-02-03\", \"2 2020-04-05 2020-08-04\", \"3 2019-10-09 2020-06-11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sc.parallelize(data).map(lambda x : \" \".join(x.split(\" \")[1:]) ).flatMap( lambda strs : re.split(\"\\\\s+\",strs)).sortBy(keyfunc=(lambda x:x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.map(lambda x: (x, )).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('_1','value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     value|       new|\n",
      "+----------+----------+\n",
      "|2019-03-04|2019-10-09|\n",
      "|2019-10-09|2020-02-03|\n",
      "|2020-02-03|2020-04-05|\n",
      "|2020-04-05|2020-06-11|\n",
      "|2020-06-11|2020-08-04|\n",
      "|2020-08-04|2020-08-04|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DSL\n",
    "df.withColumn(\"new\", F.max(\"value\").over(Window.orderBy(\"value\").rowsBetween(0,1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     value| maxvalues|\n",
      "+----------+----------+\n",
      "|2019-03-04|2019-10-09|\n",
      "|2019-10-09|2020-02-03|\n",
      "|2020-02-03|2020-04-05|\n",
      "|2020-04-05|2020-06-11|\n",
      "|2020-06-11|2020-08-04|\n",
      "|2020-08-04|2020-08-04|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL\n",
    "spark.sql(\"\"\"\n",
    "select value,max(value) over (order by value rows between current row and 1 following) maxvalues\n",
    " from t1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN/KMEAN实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4、使用鸢尾花数据集实现KNN算法**\n",
    "\n",
    "\n",
    "\n",
    "**5、使用鸢尾花数据集实现KMeans算法**\n",
    "\n",
    "备注：\n",
    "\n",
    "- 4、5的详细说明请参考视频说明\n",
    "- 文件位置：data/Iris.csv\n",
    "- 请按视频说明在原始数据集的基础上组织文件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from pyspark.sql.functions import udf, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入iris数据集\n",
    "iris = datasets.load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果：[2]\n"
     ]
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors = 3)\n",
    "neigh.fit(iris.data,iris.target)\n",
    "print(\"预测结果：\"+str(neigh.predict([[5,3,5,2]]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kemeans:[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n",
      " 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 0 2\n",
      " 2 0]\n",
      "target:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters = 3,random_state = 0).fit(iris.data)\n",
    "print(\"kemeans:\"+str(kmeans.labels_)) #打印聚类结果\n",
    "print(\"target:\"+str(iris.target)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
